{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING RSA CLUSTER LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses ANN and KNN to predict RSA cluster labels. In here, we try 50 models from 10 different seeds and chose the best according to validation accuracy and loss. Moreover, we perform 10 repeat 10-fold cross validation to ensure the model works well across a range of different training data folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING NECESSARY PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fragsys_ml import *\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READING INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"./../\"\n",
    "results_dir = os.path.join(main_dir, \"results\")\n",
    "figs_dir = os.path.join(main_dir, \"figs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_cluster_dict = load_pickle(os.path.join(results_dir, \"rsa_cluster_dict.pkl\"))\n",
    "cluster_membership = load_pickle(os.path.join(results_dir, \"rsa_cluster_membership.pkl\"))\n",
    "rsa_profs = load_pickle(os.path.join(results_dir, \"rsa_profs.pkl\"))\n",
    "ud_mat = pd.read_pickle(os.path.join(results_dir, \"UD_df.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING FEATURE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(v) for v in rsa_profs.values()])\n",
    "bs_vectors = []\n",
    "bs_vectors_dict = {}\n",
    "for bs_id, rsa_sig in rsa_profs.items():\n",
    "    rsa_sig_len = len(rsa_sig)\n",
    "    rsa_range_prop = [0 for i in range(10)] # now let us change to 10\n",
    "    for rsa in rsa_sig:\n",
    "        prop_i = int(rsa/10) # 10 RSA BINS: b1 = [0,10), b2 = [10, 20), ... b10 = [90, MAX)\n",
    "        if prop_i > 9: # if greater than 100, put in 10th bin\n",
    "            prop_i = 9\n",
    "        #print(prop_i)\n",
    "        rsa_range_prop[prop_i] += 1\n",
    "    rsa_range_prop = [round(i/rsa_sig_len, 3) for i in rsa_range_prop]\n",
    "    rsa_range_prop.insert(0, rsa_sig_len/max_len) # ADDING BINDING SITE SIZE RELATIVE TO MAX SITE SIZE (IN THIS CASE 40)\n",
    "    bs_vectors.append(rsa_range_prop)\n",
    "    bs_vectors_dict[bs_id] = rsa_range_prop\n",
    "    \n",
    "vector_df = pd.DataFrame(bs_vectors, index = list(rsa_profs.keys())) # obtaining RSA vectors, which are the 11-element features used for the machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H0Y4R8_0_BS0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O43809_0_BS0</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O43809_0_BS1</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0      1      2      3      4      5      6      7      8   \\\n",
       "H0Y4R8_0_BS0  0.150  0.000  0.000  0.167  0.167  0.333  0.000  0.167  0.167   \n",
       "O43809_0_BS0  0.600  0.375  0.250  0.083  0.167  0.042  0.042  0.000  0.042   \n",
       "O43809_0_BS1  0.325  0.231  0.077  0.231  0.077  0.077  0.077  0.077  0.154   \n",
       "\n",
       "               9    10  \n",
       "H0Y4R8_0_BS0  0.0  0.0  \n",
       "O43809_0_BS0  0.0  0.0  \n",
       "O43809_0_BS1  0.0  0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_df.to_pickle(os.path.join(results_dir, \"rsa_vectors.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame.from_dict(rsa_cluster_dict, orient = \"index\", columns = [\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = label_df - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.16\n",
       "1    0.43\n",
       "2    0.31\n",
       "3    0.10\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(label_df.label.value_counts().sort_index()/len(label_df),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H0Y4R8_0_BS0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O43809_0_BS0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O43809_0_BS1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "H0Y4R8_0_BS0      3\n",
       "O43809_0_BS0      0\n",
       "O43809_0_BS1      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vector_df.sort_index() # here, X is still a dataframe. Thus, we maintain binding site IDs\n",
    "y = label_df.sort_index()    # here, y is still a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA SPLIT IN MODEL (10/11) (USED FOR TRAINING) AND BLIND (USED FOR FINAL TESTING) (1/11) STRATIFYING BY CLUSTER LABEL, SO THEY ARE BALANCED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model, X_blind, y_model, y_blind = train_test_split(X, y, test_size = 0.091, random_state = 12345, shuffle = True, stratify = y) ### X_blind and y_blind will be the blind test at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump_pickle(X_model.index.tolist(), os.path.join(results_dir, \"X_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P47811_0_BS1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P0DTD1_0_BS23</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q96HY7_0_BS0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label\n",
       "P47811_0_BS1       1\n",
       "P0DTD1_0_BS23      1\n",
       "Q96HY7_0_BS0       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_model.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLASSES ARE IMBALANCED DUE TO OUR TRAINING DATASET, BUT BOTH TRAINING AND TEST DATA HAVE THE SAME PROPORTION FOR EACH CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "27\n",
      "266\n",
      "27\n",
      "0 0.16\n",
      "1 0.43\n",
      "2 0.31\n",
      "3 0.1\n",
      "0 0.15\n",
      "1 0.44\n",
      "2 0.3\n",
      "3 0.11\n"
     ]
    }
   ],
   "source": [
    "print(len(X_model))\n",
    "print(len(X_blind))\n",
    "print(len(y_model))\n",
    "print(len(y_blind))\n",
    "for el in [0,1,2,3]:\n",
    "    print(el, round(y_model.label.tolist().count(el)/len(y_model),2))\n",
    "for el in [0,1,2,3]:\n",
    "    print(el, round(y_blind.label.tolist().count(el)/len(y_blind),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CALLBACK TO GET BEST MODEL DURING TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', min_delta = 0.01, patience = 100, verbose = 1, mode = 'max', restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OBTAINING CLASS WEIGHTS TO MINIMISE EFFECT OF CLASS IMBALANCE IN TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                 classes = np.unique(y_model.label.tolist()),\n",
    "                                                 y = np.array(y_model.label))\n",
    "class_weights_dict = {i: class_weights[i] for i in np.unique(y_model)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.58333333, 0.57826087, 0.80120482, 2.55769231])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE WEIGHT FOR EACH CLASS IS INVERSELY PROPORTIONAL TO THEIR FREQUENCY IN THE DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.58\n",
      "1 0.58\n",
      "2 0.8\n",
      "3 2.56\n"
     ]
    }
   ],
   "source": [
    "for k, v in class_weights_dict.items():\n",
    "    print(k, round(v, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA SPLIT IN MODEL_TRAIN (0.8) (USED FOR MODEL TRAINING) AND MODEL_VAL (0.2) (USED FOR MODEL VALIDATION DURING TRAINING) STRATIFYING BY CLUSTER LABEL, SO THEY ARE BALANCED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model_train, X_model_val, y_model_train, y_model_val = train_test_split(X_model, y_model, test_size = 0.2, random_state = 12345, shuffle = True, stratify = y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(X_model_train.index.tolist()).intersection(set(y_model_train.index.tolist())))/len(y_model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(X_model_val.index.tolist()).intersection(set(y_model_val.index.tolist())))/len(y_model_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING AND VALIDATION DATA HAVE THE SAME PROPORTION FOR EACH CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "54\n",
      "212\n",
      "54\n",
      "0 0.16\n",
      "1 0.43\n",
      "2 0.31\n",
      "3 0.1\n",
      "0 0.17\n",
      "1 0.43\n",
      "2 0.31\n",
      "3 0.09\n"
     ]
    }
   ],
   "source": [
    "print(len(X_model_train))\n",
    "print(len(X_model_val))\n",
    "print(len(y_model_train))\n",
    "print(len(y_model_val))\n",
    "for el in [0,1,2,3]:\n",
    "    print(el, round(y_model_train.label.tolist().count(el)/len(y_model_train),2))\n",
    "for el in [0,1,2,3]:\n",
    "    print(el, round(y_model_val.label.tolist().count(el)/len(y_model_val),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING 100 DIFFERENT SEEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [random.randint(1, 100000) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(set(seeds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17612,\n",
       " 74607,\n",
       " 8272,\n",
       " 33433,\n",
       " 15456,\n",
       " 64938,\n",
       " 99741,\n",
       " 58916,\n",
       " 61899,\n",
       " 85406,\n",
       " 49757,\n",
       " 27520,\n",
       " 12303,\n",
       " 63945,\n",
       " 3716,\n",
       " 51094,\n",
       " 56724,\n",
       " 79619,\n",
       " 99914,\n",
       " 277,\n",
       " 91205,\n",
       " 58378,\n",
       " 34909,\n",
       " 94574,\n",
       " 29985,\n",
       " 77484,\n",
       " 13400,\n",
       " 41607,\n",
       " 4010,\n",
       " 2926,\n",
       " 3336,\n",
       " 85138,\n",
       " 70965,\n",
       " 1207,\n",
       " 49966,\n",
       " 89979,\n",
       " 28391,\n",
       " 55328,\n",
       " 95139,\n",
       " 3807,\n",
       " 69158,\n",
       " 29058,\n",
       " 57395,\n",
       " 64988,\n",
       " 72465,\n",
       " 30551,\n",
       " 45312,\n",
       " 30261,\n",
       " 88716,\n",
       " 28677,\n",
       " 99739,\n",
       " 60242,\n",
       " 37983,\n",
       " 2817,\n",
       " 54550,\n",
       " 72936,\n",
       " 84187,\n",
       " 13108,\n",
       " 24368,\n",
       " 82491,\n",
       " 94849,\n",
       " 38849,\n",
       " 15846,\n",
       " 97406,\n",
       " 43608,\n",
       " 94567,\n",
       " 93218,\n",
       " 65641,\n",
       " 55327,\n",
       " 66548,\n",
       " 87859,\n",
       " 24884,\n",
       " 39764,\n",
       " 37246,\n",
       " 77016,\n",
       " 65453,\n",
       " 66229,\n",
       " 51558,\n",
       " 77202,\n",
       " 4526,\n",
       " 62945,\n",
       " 31817,\n",
       " 97483,\n",
       " 52991,\n",
       " 54305,\n",
       " 87130,\n",
       " 22677,\n",
       " 48120,\n",
       " 71933,\n",
       " 92149,\n",
       " 88407,\n",
       " 96760,\n",
       " 49114,\n",
       " 11334,\n",
       " 57536,\n",
       " 87001,\n",
       " 66641,\n",
       " 14147,\n",
       " 21457,\n",
       " 68281]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeds = \n",
    "# [\n",
    "#     17612, 74607, 8272, 33433, 15456, 64938, 99741, 58916, 61899, 85406,\n",
    "#     49757, 27520, 12303, 63945, 3716, 51094, 56724, 79619, 99914, 277,\n",
    "#     91205, 58378, 34909, 94574, 29985, 77484, 13400, 41607, 4010, 2926,\n",
    "#     3336, 85138, 70965, 1207, 49966, 89979, 28391, 55328, 95139, 3807,\n",
    "#     69158, 29058, 57395, 64988, 72465, 30551, 45312, 30261, 88716, 28677,\n",
    "#     99739, 60242, 37983, 2817, 54550, 72936, 84187, 13108, 24368, 82491,\n",
    "#     94849, 38849, 15846, 97406, 43608, 94567, 93218, 65641, 55327, 66548,\n",
    "#     87859, 24884, 39764, 37246, 77016, 65453, 66229, 51558, 77202, 4526,\n",
    "#     62945, 31817, 97483, 52991, 54305, 87130, 22677, 48120, 71933, 92149,\n",
    "#     88407, 96760, 49114, 11334, 57536, 87001, 66641, 14147, 21457, 68281\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.GlorotUniform(seed = 3551)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P0DTD1_2_BS0</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q6PJP8_0_BS9</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q8IU60_0_BS0</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0      1      2      3      4      5      6      7      8   \\\n",
       "P0DTD1_2_BS0  0.550  0.136  0.227  0.045  0.182  0.091  0.000  0.136  0.045   \n",
       "Q6PJP8_0_BS9  0.175  0.000  0.000  0.429  0.143  0.000  0.000  0.286  0.143   \n",
       "Q8IU60_0_BS0  0.300  0.333  0.000  0.167  0.250  0.083  0.083  0.000  0.000   \n",
       "\n",
       "                 9      10  \n",
       "P0DTD1_2_BS0  0.091  0.045  \n",
       "Q6PJP8_0_BS9  0.000  0.000  \n",
       "Q8IU60_0_BS0  0.000  0.083  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_model_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.path.join(main_dir, \"ANN_results/params_explore\")\n",
    "if not os.path.isdir(wd):\n",
    "    os.mkdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(seeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURRENT ML SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING SEED 17612\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 136: early stopping\n",
      "STARTING SEED 74607\n",
      "Restoring model weights from the end of the best epoch: 86.\n",
      "Epoch 186: early stopping\n",
      "STARTING SEED 8272\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Epoch 131: early stopping\n",
      "STARTING SEED 33433\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Epoch 142: early stopping\n",
      "STARTING SEED 15456\n",
      "Restoring model weights from the end of the best epoch: 98.\n",
      "Epoch 198: early stopping\n",
      "STARTING SEED 64938\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Epoch 146: early stopping\n",
      "STARTING SEED 99741\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Epoch 137: early stopping\n",
      "STARTING SEED 58916\n",
      "Restoring model weights from the end of the best epoch: 71.\n",
      "Epoch 171: early stopping\n",
      "STARTING SEED 61899\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 139: early stopping\n",
      "STARTING SEED 85406\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Epoch 140: early stopping\n",
      "STARTING SEED 49757\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 169: early stopping\n",
      "STARTING SEED 27520\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 152: early stopping\n",
      "STARTING SEED 12303\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 147: early stopping\n",
      "STARTING SEED 63945\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 132: early stopping\n",
      "STARTING SEED 3716\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 136: early stopping\n",
      "STARTING SEED 51094\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 136: early stopping\n",
      "STARTING SEED 56724\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 138: early stopping\n",
      "STARTING SEED 79619\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "Epoch 151: early stopping\n",
      "STARTING SEED 99914\n",
      "Restoring model weights from the end of the best epoch: 61.\n",
      "Epoch 161: early stopping\n",
      "STARTING SEED 277\n",
      "Restoring model weights from the end of the best epoch: 63.\n",
      "Epoch 163: early stopping\n",
      "STARTING SEED 91205\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 138: early stopping\n",
      "STARTING SEED 58378\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 152: early stopping\n",
      "STARTING SEED 34909\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 156: early stopping\n",
      "STARTING SEED 94574\n",
      "Restoring model weights from the end of the best epoch: 87.\n",
      "Epoch 187: early stopping\n",
      "STARTING SEED 29985\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 148: early stopping\n",
      "STARTING SEED 77484\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Epoch 141: early stopping\n",
      "STARTING SEED 13400\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 152: early stopping\n",
      "STARTING SEED 41607\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 152: early stopping\n",
      "STARTING SEED 4010\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Epoch 135: early stopping\n",
      "STARTING SEED 2926\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "Epoch 151: early stopping\n",
      "STARTING SEED 3336\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 147: early stopping\n",
      "STARTING SEED 85138\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "Epoch 151: early stopping\n",
      "STARTING SEED 70965\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "Epoch 180: early stopping\n",
      "STARTING SEED 1207\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 147: early stopping\n",
      "STARTING SEED 49966\n",
      "Restoring model weights from the end of the best epoch: 77.\n",
      "Epoch 177: early stopping\n",
      "STARTING SEED 89979\n",
      "Restoring model weights from the end of the best epoch: 53.\n",
      "Epoch 153: early stopping\n",
      "STARTING SEED 28391\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 143: early stopping\n",
      "STARTING SEED 55328\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 157: early stopping\n",
      "STARTING SEED 95139\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Epoch 137: early stopping\n",
      "STARTING SEED 3807\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 134: early stopping\n",
      "STARTING SEED 69158\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Epoch 141: early stopping\n",
      "STARTING SEED 29058\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 138: early stopping\n",
      "STARTING SEED 57395\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Epoch 130: early stopping\n",
      "STARTING SEED 64988\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 139: early stopping\n",
      "STARTING SEED 72465\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Epoch 140: early stopping\n",
      "STARTING SEED 30551\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 138: early stopping\n",
      "STARTING SEED 45312\n",
      "Restoring model weights from the end of the best epoch: 55.\n",
      "Epoch 155: early stopping\n",
      "STARTING SEED 30261\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 160: early stopping\n",
      "STARTING SEED 88716\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 132: early stopping\n",
      "STARTING SEED 28677\n",
      "Restoring model weights from the end of the best epoch: 69.\n",
      "Epoch 169: early stopping\n",
      "STARTING SEED 99739\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 134: early stopping\n",
      "STARTING SEED 60242\n",
      "Restoring model weights from the end of the best epoch: 59.\n",
      "Epoch 159: early stopping\n",
      "STARTING SEED 37983\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Epoch 150: early stopping\n",
      "STARTING SEED 2817\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 170: early stopping\n",
      "STARTING SEED 54550\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "Epoch 175: early stopping\n",
      "STARTING SEED 72936\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Epoch 144: early stopping\n",
      "STARTING SEED 84187\n",
      "Restoring model weights from the end of the best epoch: 118.\n",
      "Epoch 218: early stopping\n",
      "STARTING SEED 13108\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "Epoch 157: early stopping\n",
      "STARTING SEED 24368\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 156: early stopping\n",
      "STARTING SEED 82491\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 152: early stopping\n",
      "STARTING SEED 94849\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 148: early stopping\n",
      "STARTING SEED 38849\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Epoch 141: early stopping\n",
      "STARTING SEED 15846\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Epoch 142: early stopping\n",
      "STARTING SEED 97406\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 148: early stopping\n",
      "STARTING SEED 43608\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 147: early stopping\n",
      "STARTING SEED 94567\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Epoch 145: early stopping\n",
      "STARTING SEED 93218\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Epoch 146: early stopping\n",
      "STARTING SEED 65641\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Epoch 137: early stopping\n",
      "STARTING SEED 55327\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 152: early stopping\n",
      "STARTING SEED 66548\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 147: early stopping\n",
      "STARTING SEED 87859\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 156: early stopping\n",
      "STARTING SEED 24884\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Epoch 147: early stopping\n",
      "STARTING SEED 39764\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Epoch 145: early stopping\n",
      "STARTING SEED 37246\n",
      "Restoring model weights from the end of the best epoch: 91.\n",
      "Epoch 191: early stopping\n",
      "STARTING SEED 77016\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 138: early stopping\n",
      "STARTING SEED 65453\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Epoch 149: early stopping\n",
      "STARTING SEED 66229\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 166: early stopping\n",
      "STARTING SEED 51558\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 143: early stopping\n",
      "STARTING SEED 77202\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 148: early stopping\n",
      "STARTING SEED 4526\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 148: early stopping\n",
      "STARTING SEED 62945\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Epoch 145: early stopping\n",
      "STARTING SEED 31817\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 148: early stopping\n",
      "STARTING SEED 97483\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 139: early stopping\n",
      "STARTING SEED 52991\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 152: early stopping\n",
      "STARTING SEED 54305\n",
      "Restoring model weights from the end of the best epoch: 54.\n",
      "Epoch 154: early stopping\n",
      "STARTING SEED 87130\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "Epoch 152: early stopping\n",
      "STARTING SEED 22677\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 132: early stopping\n",
      "STARTING SEED 48120\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Epoch 150: early stopping\n",
      "STARTING SEED 71933\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Epoch 144: early stopping\n",
      "STARTING SEED 92149\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "Epoch 164: early stopping\n",
      "STARTING SEED 88407\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 139: early stopping\n",
      "STARTING SEED 96760\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Epoch 135: early stopping\n",
      "STARTING SEED 49114\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 136: early stopping\n",
      "STARTING SEED 11334\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Epoch 140: early stopping\n",
      "STARTING SEED 57536\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Epoch 138: early stopping\n",
      "STARTING SEED 87001\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Epoch 145: early stopping\n",
      "STARTING SEED 66641\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 132: early stopping\n",
      "STARTING SEED 14147\n",
      "Restoring model weights from the end of the best epoch: 60.\n",
      "Epoch 160: early stopping\n",
      "STARTING SEED 21457\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Epoch 141: early stopping\n",
      "STARTING SEED 68281\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 148: early stopping\n"
     ]
    }
   ],
   "source": [
    "param_folder = os.path.join(wd, \"current\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING HIDDEN LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING SEED 17612\n",
      "Restoring model weights from the end of the best epoch: 123.\n",
      "Epoch 223: early stopping\n",
      "STARTING SEED 74607\n",
      "Restoring model weights from the end of the best epoch: 123.\n",
      "Epoch 223: early stopping\n",
      "STARTING SEED 8272\n",
      "Restoring model weights from the end of the best epoch: 159.\n",
      "Epoch 259: early stopping\n",
      "STARTING SEED 33433\n",
      "STARTING SEED 15456\n",
      "STARTING SEED 64938\n",
      "STARTING SEED 99741\n",
      "STARTING SEED 58916\n",
      "STARTING SEED 61899\n",
      "Restoring model weights from the end of the best epoch: 74.\n",
      "Epoch 174: early stopping\n",
      "STARTING SEED 85406\n",
      "STARTING SEED 49757\n",
      "STARTING SEED 27520\n",
      "STARTING SEED 12303\n",
      "STARTING SEED 63945\n",
      "Restoring model weights from the end of the best epoch: 171.\n",
      "Epoch 271: early stopping\n",
      "STARTING SEED 3716\n",
      "STARTING SEED 51094\n",
      "STARTING SEED 56724\n",
      "STARTING SEED 79619\n",
      "STARTING SEED 99914\n",
      "Restoring model weights from the end of the best epoch: 191.\n",
      "Epoch 291: early stopping\n",
      "STARTING SEED 277\n",
      "STARTING SEED 91205\n",
      "Restoring model weights from the end of the best epoch: 160.\n",
      "Epoch 260: early stopping\n",
      "STARTING SEED 58378\n",
      "Restoring model weights from the end of the best epoch: 193.\n",
      "Epoch 293: early stopping\n",
      "STARTING SEED 34909\n",
      "STARTING SEED 94574\n",
      "STARTING SEED 29985\n",
      "Restoring model weights from the end of the best epoch: 133.\n",
      "Epoch 233: early stopping\n",
      "STARTING SEED 77484\n",
      "Restoring model weights from the end of the best epoch: 190.\n",
      "Epoch 290: early stopping\n",
      "STARTING SEED 13400\n",
      "STARTING SEED 41607\n",
      "STARTING SEED 4010\n",
      "STARTING SEED 2926\n",
      "STARTING SEED 3336\n",
      "STARTING SEED 85138\n",
      "Restoring model weights from the end of the best epoch: 195.\n",
      "Epoch 295: early stopping\n",
      "STARTING SEED 70965\n",
      "Restoring model weights from the end of the best epoch: 145.\n",
      "Epoch 245: early stopping\n",
      "STARTING SEED 1207\n",
      "STARTING SEED 49966\n",
      "STARTING SEED 89979\n",
      "STARTING SEED 28391\n",
      "STARTING SEED 55328\n",
      "STARTING SEED 95139\n",
      "STARTING SEED 3807\n",
      "STARTING SEED 69158\n",
      "STARTING SEED 29058\n",
      "STARTING SEED 57395\n",
      "Restoring model weights from the end of the best epoch: 198.\n",
      "Epoch 298: early stopping\n",
      "STARTING SEED 64988\n",
      "STARTING SEED 72465\n",
      "STARTING SEED 30551\n",
      "Restoring model weights from the end of the best epoch: 158.\n",
      "Epoch 258: early stopping\n",
      "STARTING SEED 45312\n",
      "Restoring model weights from the end of the best epoch: 173.\n",
      "Epoch 273: early stopping\n",
      "STARTING SEED 30261\n",
      "Restoring model weights from the end of the best epoch: 153.\n",
      "Epoch 253: early stopping\n",
      "STARTING SEED 88716\n",
      "STARTING SEED 28677\n",
      "STARTING SEED 99739\n",
      "Restoring model weights from the end of the best epoch: 157.\n",
      "Epoch 257: early stopping\n",
      "STARTING SEED 60242\n",
      "STARTING SEED 37983\n",
      "STARTING SEED 2817\n",
      "STARTING SEED 54550\n",
      "Restoring model weights from the end of the best epoch: 158.\n",
      "Epoch 258: early stopping\n",
      "STARTING SEED 72936\n",
      "STARTING SEED 84187\n",
      "STARTING SEED 13108\n",
      "Restoring model weights from the end of the best epoch: 148.\n",
      "Epoch 248: early stopping\n",
      "STARTING SEED 24368\n",
      "STARTING SEED 82491\n",
      "STARTING SEED 94849\n",
      "STARTING SEED 38849\n",
      "Restoring model weights from the end of the best epoch: 160.\n",
      "Epoch 260: early stopping\n",
      "STARTING SEED 15846\n",
      "STARTING SEED 97406\n",
      "STARTING SEED 43608\n",
      "STARTING SEED 94567\n",
      "STARTING SEED 93218\n",
      "Restoring model weights from the end of the best epoch: 142.\n",
      "Epoch 242: early stopping\n",
      "STARTING SEED 65641\n",
      "STARTING SEED 55327\n",
      "Restoring model weights from the end of the best epoch: 161.\n",
      "Epoch 261: early stopping\n",
      "STARTING SEED 66548\n",
      "Restoring model weights from the end of the best epoch: 156.\n",
      "Epoch 256: early stopping\n",
      "STARTING SEED 87859\n",
      "STARTING SEED 24884\n"
     ]
    }
   ],
   "source": [
    "param_folder = os.path.join(wd, \"no_layer\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        #Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"2_layer\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"5_layer\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"10_layer\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 10, activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"1_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 1, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"2_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 2, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"3_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 3, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"4_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 4, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"5_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 5, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"6_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 6, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"7_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 7, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"8_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 8, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"9_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 9, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"11_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 11, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"20_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 20, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"25_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 25, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"50_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 50, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"100_neurons\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 100, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"sigmoid\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"sigmoid\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"elu\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"elu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"selu\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"selu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"exp\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"exponential\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"tanh\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"tanh\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softplus activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"softplus\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"softplus\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"softmax\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"softmax\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softsign activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"softsign\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"softsign\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# He Normal initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"initialiser_he_norm\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.HeNormal(seed=seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# He Uniform initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"initialiser_he_uni\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.HeUniform(seed=seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Normal initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"initialiser_rand_norm\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Uniform initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"initialiser_rand_uni\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Normal initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"initialiser_trunc_norm\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ones initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"initialiser_ones\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.Ones()\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeros initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"initialiser_zeros\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.Zeros()\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glorot Normal initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"initialiser_glorot_normal\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotNormal(seed=seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean squared error loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"MSE_loss_func\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"mean_squared_error\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"poisson_loss_func\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"poisson\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"kld_loss_func\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"kl_divergence\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"optimiser_adadelta\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=keras.optimizers.Adadelta(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"optimiser_adagrad\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=keras.optimizers.Adagrad(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"optimiser_adamax\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=keras.optimizers.Adamax(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"optimiser_nadam\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser Ftrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"optimiser_ftrl\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=keras.optimizers.Ftrl(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"optimiser_SGD\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"optimiser_RMSProp\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"learning_rate_0.001\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.001), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"learning_rate_0.005\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.005), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"learning_rate_0.05\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.05), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"learning_rate_0.1\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.1), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"learning_rate_0.25\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.25), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"learning_rate_0.5\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.5), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"learning_rate_1.0\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=1.0), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layer (Rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"dropout_0.1\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dropout(0.1),\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layer (Rate = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"dropout_0.25\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dropout(0.25),\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layer (Rate = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"dropout_0.33\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dropout(0.3333),\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layer (Rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"dropout_0.5\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dropout(0.5),\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layer (Rate = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"dropout_0.75\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed), # THIS IS THE MODEL\n",
    "        Dropout(0.75),\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 regulariser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"L1_regular\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed, kernel_regularizer='l1'), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"L2_regular\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed, kernel_regularizer='l2'), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"L1L2_regular\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed, kernel_regularizer='l1_l2'), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal regularisation (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"ortho_rows_regular\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "    \n",
    "    regu = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01, mode=\"rows\")\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed, kernel_regularizer=regu), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal regularisation (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"ortho_cols_regular\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "    \n",
    "    regu = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01, mode=\"columns\")\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed, kernel_regularizer=regu), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All L1 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"all_L1_regular\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "    \n",
    "    #regu = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01, mode=\"columns\")\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed, kernel_regularizer='l1', bias_regularizer='l1', activity_regularizer='l1'), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"all_L2_regular\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "    \n",
    "    #regu = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01, mode=\"columns\")\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed, kernel_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2'), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All L1L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_folder = os.path.join(wd, \"all_L1L2_regular\")\n",
    "\n",
    "if not os.path.isdir(param_folder):\n",
    "    os.mkdir(param_folder)\n",
    "    \n",
    "for seed in seeds:\n",
    "    print(\"STARTING SEED {}\".format(seed))\n",
    "    \n",
    "    mods_dir = os.path.join(param_folder, \"models\")\n",
    "    hist_dir = os.path.join(param_folder, \"hists\")\n",
    "    \n",
    "    if not os.path.isdir(mods_dir):\n",
    "        os.mkdir(mods_dir)\n",
    "    if not os.path.isdir(hist_dir):\n",
    "        os.mkdir(hist_dir)\n",
    "    \n",
    "    initializer_seed = tf.keras.initializers.GlorotUniform(seed = seed)\n",
    "    \n",
    "    #regu = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01, mode=\"columns\")\n",
    "   \n",
    "    model_i = Sequential([\n",
    "        Dense(units = 10, input_shape=(11,), activation = \"relu\", kernel_initializer = initializer_seed, kernel_regularizer='l1_l2', bias_regularizer='l1_l2', activity_regularizer='l1_l2'), # THIS IS THE MODEL\n",
    "        Dense(units = 4, activation = \"softmax\", kernel_initializer = initializer_seed)\n",
    "    ])\n",
    "\n",
    "    model_i.compile(optimizer=Adam(learning_rate=0.01), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # COMPILE MODEL\n",
    "\n",
    "    hist_i = model_i.fit(\n",
    "        x = X_model_train, y = y_model_train, batch_size = 32,\n",
    "        callbacks = [callback], validation_data = (X_model_val, y_model_val),\n",
    "        epochs = 300, shuffle = True, verbose = 0, class_weight =  class_weights_dict # TRAIN MODEL\n",
    "    )\n",
    "    final_epoch = callback.stopped_epoch-100\n",
    "    if final_epoch == -100:\n",
    "        final_epoch = 299\n",
    "    final_acc = round(hist_i.history[\"val_accuracy\"][final_epoch],2)\n",
    "\n",
    "    model_i.save(os.path.join(mods_dir, \"{}_model_epoch_{}_train_acc_{}.h5\".format(seed, final_epoch, round(final_acc, 2)))) # SAVING MODEL\n",
    "\n",
    "    pd.DataFrame(hist_i.history).to_csv(os.path.join(hist_dir, \"{}_hist_epoch_{}_train_acc_{}.csv\".format(seed, final_epoch, round(final_acc, 2))), index = False) # SAVING TRAINING DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_params = [\n",
    "    \"current\", \"no_layer\", \"2_layer\", \"5_layer\", \"10_layer\",\n",
    "    \"1_neurons\", \"2_neurons\", \"3_neurons\", \"4_neurons\", \"5_neurons\",\n",
    "    \"6_neurons\", \"7_neurons\", \"8_neurons\", \"9_neurons\",\n",
    "    \"11_neurons\", \"20_neurons\", \"50_neurons\", \"100_neurons\",\n",
    "    \"dropout_0.1\", \"dropout_0.25\", \"dropout_0.33\", \"dropout_0.5\", \"dropout_0.75\",\n",
    "    \"L1_regular\", \"L2_regular\", \"L1L2_regular\", \"ortho_rows_regular\", \"ortho_cols_regular\",\n",
    "    \"all_L1_regular\", \"all_L2_regular\", \"all_L1L2_regular\",\n",
    "    \"sigmoid\", \"tanh\", \"elu\", \"selu\", \"exp\", \"softplus\", \"softmax\", \"softsign\",\n",
    "    \"initialiser_he_norm\", \"initialiser_he_uni\", \"initialiser_rand_norm\", \"initialiser_rand_uni\",\n",
    "    \"initialiser_trunc_norm\", \"initialiser_ones\", \"initialiser_zeros\", \"initialiser_glorot_normal\",\n",
    "    \"MSE_loss_func\", \"poisson_loss_func\", \"kld_loss_func\", \"optimiser_SGD\", \"optimiser_RMSProp\",\n",
    "    \"optimiser_adadelta\", \"optimiser_adagrad\", \"optimiser_adamax\", \"optimiser_nadam\", \"optimiser_ftrl\",\n",
    "    \"learning_rate_0.001\", \"learning_rate_0.005\", \"learning_rate_0.05\", \"learning_rate_0.1\",\n",
    "    \"learning_rate_0.25\", \"learning_rate_0.5\", \"learning_rate_1.0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"\"\"(\\d+)_hist_epoch_(\\d+)_train_acc_(\\d+\\.\\d+)\\.csv\"\"\")\n",
    "\n",
    "versions, seed_ids, n_epochs, accs = [[], [], [], []]\n",
    "\n",
    "for param in diff_params:\n",
    "    param_dir = os.path.join(wd, param)\n",
    "    hists_dir = os.path.join(param_dir, \"hists\")\n",
    "    hists = os.listdir(hists_dir)\n",
    "    for hist in hists:\n",
    "        #print(hist)\n",
    "        try:\n",
    "            m = p.match(hist)\n",
    "            if m:\n",
    "                g = m.groups()\n",
    "                seed, epoch, acc = g\n",
    "        except:\n",
    "            print(\"ERROR\")\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(hists_dir, hist))\n",
    "       # print(seed, epoch, acc)\n",
    "        versions.append(param)\n",
    "        seed_ids.append(seed)\n",
    "        n_epochs.append(int(epoch))\n",
    "        accs.append(df.loc[int(epoch), \"val_accuracy\"])\n",
    "        #break\n",
    "    #break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(list(zip(versions, seed_ids, n_epochs, accs)),\n",
    "              columns=['version','seed', 'f_epoch', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5), dpi = 150)\n",
    "\n",
    "# palette = {\n",
    "#     'ANN': 'firebrick',\n",
    "#     'KNN': 'royalblue',\n",
    "#     'Random ANN': 'tomato',\n",
    "#     'Random KNN': 'cornflowerblue',\n",
    "#     'Random': 'gold'\n",
    "# }\n",
    "\n",
    "# alpha = 0.75\n",
    "\n",
    "# PROPS = {\n",
    "#     'boxprops':{'edgecolor':'k', 'alpha' : alpha},\n",
    "#     'medianprops':{'color':'k'},\n",
    "#     'whiskerprops':{'color':'k'},\n",
    "#     'capprops':{'color':'k'},\n",
    "    \n",
    "# }\n",
    "\n",
    "flierprops = dict(marker='o', markerfacecolor= \"k\", markersize=5,\n",
    "                  linestyle='none', color=\"k\", markeredgecolor='k')\n",
    "\n",
    "# Create the boxplot with different colors\n",
    "sns.boxplot(x='version', y='acc', data=master_df)\n",
    "\n",
    "plt.ylim(-0.05, 1.1)\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.xticks(rotation=-90)\n",
    "plt.xlabel(\"Model\")\n",
    "\n",
    "#plt.savefig(os.path.join(figs_dir, \"ML_CV_rev.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_pickle(os.path.join(wd, \"ANN_parameter_exploration.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
